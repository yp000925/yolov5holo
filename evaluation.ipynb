{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients, 16.3 GFLOPS\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/ypzhang/exp/comparison/labels/test_02' images and labels... 900 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 900/900 [00:00<00:00, 1746.94it/s]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /home/ypzhang/exp/comparison/labels/test_02.cache\n",
      "100%|██████████| 57/57 [00:39<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Accuracy@0.08mm     Accuracy@0.39mm     Accuracy@0.78mm   Accuracy@1.5625mm   Accuracy@2.3438mm\n",
      "              32.09%              71.04%              76.49%              80.38%              82.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from test import pred_label_onehot\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import create_dataloader,create_dataloader_modified\n",
    "from utils.general import coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements, \\\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr,post_nms, nms_modified,nms_depthmap\n",
    "from utils.metrics import ap_per_class, ConfusionMatrix\n",
    "from utils.plots import plot_images, output_to_target, plot_study_txt,plot_images_modified\n",
    "from utils.torch_utils import select_device, time_synchronized\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import os\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "    source = '/home/ypzhang/exp/comparison/images/test_02'\n",
    "    weights = '/home/ypzhang/exp/yolov5holo/train/exp_bboxrescaled_weightedloss/weights/best.pt'\n",
    "    # weights = '/Users/zhangyunping/PycharmProjects/yolov5holo/train/exp_depthmap/best.pt'\n",
    "    view_image = True\n",
    "    img_size = 512\n",
    "    # project = '/content/drive/MyDrive/yoloV5/train/exp3'\n",
    "    task = 'test'\n",
    "    device = torch.device('cuda')\n",
    "    set_logging()\n",
    "    batch_size = 16\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    imgsz = check_img_size(img_size, s=gs)  # check img_size\n",
    "\n",
    "    # half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "    # if half:\n",
    "    #     model.half()\n",
    "\n",
    "    model.eval()\n",
    "    nc = 256 # number of classes\n",
    "\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    task = task if task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "    dataloader = create_dataloader_modified(source, imgsz, batch_size, gs,\n",
    "                                    pad=0.5, rect=True,\n",
    "                                   prefix=colorstr(f'{task}: '),image_weights=True)[0]\n",
    "\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "\n",
    "\n",
    "    mat = ConfusionMatrix(nc=256, conf=0, iou_thres=0.6)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader)):\n",
    "            # targets in the format [batch_idx, class_id, x,y,w,h]\n",
    "            img = img.to(device, non_blocking=True)\n",
    "            img = img.float()  # uint8 to fp16/32\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "            targets = targets.to(device)\n",
    "            targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "\n",
    "            out_, train_out = model(img)  # inference and training outputs\n",
    "\n",
    "            # # if would like to use one_hot for output\n",
    "            # out = pred_label_onehot(out)\n",
    "            # out = non_max_suppression(out)\n",
    "            # out = post_nms(out,0.45)# list of anchors with [xyxy, conf, cls]\n",
    "\n",
    "\n",
    "            # # if would like to use depthmap as the class directly\n",
    "            # out = nms_modified(out_,obj_thre=0.8, iou_thres=0.5, nc=256) # list of anchors with [xyxy, conf, cls]\n",
    "            out = nms_depthmap(out_,obj_thre=0.8, iou_thres=0.5, nc=256)\n",
    "            # 因为用了torch自带的nms所以变成了xyxy\n",
    "\n",
    "\n",
    "    # plot ----------------------------------------------------------------------------------------------------------------\n",
    "            # list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "            plot_images_modified(img, targets, paths, fname='check.jpg', names=None)\n",
    "            plot_images_modified(img, output_to_target(out),paths ,fname = 'check_pred.jpg',names=None)\n",
    "\n",
    "\n",
    "    # update confusion matrix ----------------------------------------------------------------------------------------------\n",
    "            for batch_idx in range(len(out)):\n",
    "                labels = targets[targets[:,0].int()==batch_idx][:,1::] # class, x,y,w,h\n",
    "                detections = out[batch_idx] # x,y,x,y,conf,cls\n",
    "#                 detections[:,5] = detections[:,5].int()\n",
    "                detections[:,5]  = torch.clamp(detections[:,5].int(),0,256)\n",
    "                labels[:,1::] = xywh2xyxy(labels[:,1::])# class, x,y,x,y\n",
    "                mat.process_batch(detections,labels)\n",
    "\n",
    "    # Calculate accuracy      ----------------------------------------------------------------------------------------------\n",
    "\n",
    "        mtx = mat.matrix\n",
    "        accuracies = []\n",
    "        for thred in [1, 5, 10, 20, 30]:\n",
    "             # take prediction within this range as acceptable\n",
    "            correct_match = 0\n",
    "            total_num = np.sum(mtx[:, 0:256])\n",
    "            for gt_cls in range(mtx.shape[1] - 1):\n",
    "                correct_match += np.sum(mtx[max(0, gt_cls - thred):min(gt_cls + thred, mtx.shape[0] - 1), gt_cls])\n",
    "            accuracy = correct_match / total_num\n",
    "            accuracies.append(accuracy)\n",
    "        s = ('%20s'*5) % ('Accuracy@0.08mm', 'Accuracy@0.39mm', 'Accuracy@0.78mm', 'Accuracy@1.5625mm','Accuracy@2.3438mm')\n",
    "        print(s)\n",
    "        pf = '{:20.2%}'*5\n",
    "        print(pf.format(*accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
